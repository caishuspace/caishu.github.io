---
title: "吴恩达深度学习课程 逻辑回归部分"
description: "吴恩达深度学习课程-逻辑回归部分"
keywords: "吴恩达深度学习课程,逻辑回归部分"

date: 2020-11-30T15:07:06+08:00
lastmod: 2020-11-30T15:07:06+08:00

categories:
  - 算法刷题
tags:
  - 深度学习入门
  - 逻辑回归
# categories:技术积累、论文阅读、问题解决、算法刷题、其他等四个条目，其余放在tags里面。

# 原文作者
# Post's origin author name
#author:
# 原文链接
# Post's origin link URL
#link:
# 图片链接，用在open graph和twitter卡片上
# Image source link that will use in open graph and twitter card
#imgs:
# 在首页展开内容
# Expand content on the home page
#expand: true
# 外部链接地址，访问时直接跳转
# It's means that will redirecting to external links
#extlink:
# 在当前页面开启或关闭评论功能
# Switch to enabled or disabled comment plugins in this post
#comment:
#  enable: false
# 开启文章目录功能
# Enable table of content
#toc: false
# 绝对访问路径
# Absolute link for visit
#url: "吴恩达深度学习课程-逻辑回归部分.html"
# 开启文章置顶，数字越小越靠前
# Sticky post set-top in home page and the smaller nubmer will more forward.
#weight: 1
---

吴恩达深度学习课程小记
<!--more-->

## 一些标记符号 

![深度学习符号标记1](https://s3.ax1x.com/2020/11/29/DgDoxf.jpg)

由于目前这个博客编辑器对于数学符号支持不是太好，索性直接上图
## 逻辑回归部分
![逻辑回归](https://s3.ax1x.com/2020/11/29/DgruQK.png)

#### L(y-hat,y) 损失函数
截图里面，可以使用平方差模型作为损失函数，但是会出现两个问题，就是if y=1  和y=0 部分 【会产生非凸优化函数的问题】，故选择设定函数（方框标注的L函数）作为损失函数
#### J（w,b）代价函数 
在优化逻辑回归函数时，取w,b最优，使得代价函数最小。

#### 后记部分
　　
昨天抽了个时间，把博客问题处理掉，今天就用来记录学习的一些内容吧。
　
之前开开组会，都是迷迷糊糊的，什么ReLU,simode函数，什么反向传播，正向传播，都是似懂非懂迷迷糊糊，之前准备上课讲述SVM代码，结果看了半个月，论文都还没找，一个反向传播，问师兄，我也没怎么明白，大致就是参数更新过程。
　
框架的使用，确实方便了开发，但是对于个人的发展总归还是不利的，所以，还是听从师兄的建议，从头开始，先把经典基础理论课听完！
　　
上面的内容，可能有我听得不到位的地方，甚至是我理解有误的地方，所以，文章仅供参考。仅作为个人学习成长记录之用吧。看看什么时候能淦完这个大课程！'

